{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9156046,"sourceType":"datasetVersion","datasetId":5531206},{"sourceId":9160755,"sourceType":"datasetVersion","datasetId":5534521},{"sourceId":9166531,"sourceType":"datasetVersion","datasetId":5538748}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Bert**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\nfrom torch.utils.data import Dataset\nimport torch\nimport numpy as np\n\n# Load the data\nfile_path = '/kaggle/input/bbc-data-new/balanced_news_samples_max.csv'\ndata = pd.read_csv(file_path)\n\n# Encode the sentiment labels\nlabel_map = {'POSITIVE': 0, 'NEGATIVE': 1, 'NEUTRAL': 2}\ndata['Sentiment'] = data['Sentiment'].map(label_map)\n\n# Split the dataset into training and validation sets\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(\n    data['Title'], data['Sentiment'], test_size=0.2, random_state=42)\n\n# Load the tokenizer and model\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n\n# Define custom dataset class\nclass NewsDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n        item = {key: val.squeeze(0) for key, val in encoding.items()}\n        item['labels'] = torch.tensor(label, dtype=torch.long)\n        return item\n\n# Create datasets\ntrain_dataset = NewsDataset(train_texts.tolist(), train_labels.tolist(), tokenizer)\nval_dataset = NewsDataset(val_texts.tolist(), val_labels.tolist(), tokenizer)\n\n# Set up training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=5,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=10,\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    save_strategy=\"epoch\",\n)\n\n# Define evaluation metric\ndef compute_metrics(p):\n    preds = np.argmax(p.predictions, axis=1)\n    return {\"accuracy\": (preds == p.label_ids).mean()}\n\n# Initialize the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics,\n)\n\n# Fine-tune the model\ntrainer.train()\n\n# Evaluate the model}\neval_result = trainer.evaluate()\nprint(f\"Accuracy for BERT: {eval_result['eval_accuracy']}\")\n##API KEY Wandbai 9c087fc721babdf8740dfa1e1b561057a4d9a03e","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-13T14:20:51.914542Z","iopub.execute_input":"2024-08-13T14:20:51.914838Z","iopub.status.idle":"2024-08-13T14:31:22.565027Z","shell.execute_reply.started":"2024-08-13T14:20:51.914809Z","shell.execute_reply":"2024-08-13T14:31:22.564150Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-08-13 14:20:55.660318: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-13 14:20:55.660373: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-13 14:20:55.661890: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mali408mehmood\u001b[0m (\u001b[33mali408mehmood-ghulam-ishaq-khan-institute-of-engineering\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240813_142102-2xw7twqz</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ali408mehmood-ghulam-ishaq-khan-institute-of-engineering/huggingface/runs/2xw7twqz' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/ali408mehmood-ghulam-ishaq-khan-institute-of-engineering/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ali408mehmood-ghulam-ishaq-khan-institute-of-engineering/huggingface' target=\"_blank\">https://wandb.ai/ali408mehmood-ghulam-ishaq-khan-institute-of-engineering/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ali408mehmood-ghulam-ishaq-khan-institute-of-engineering/huggingface/runs/2xw7twqz' target=\"_blank\">https://wandb.ai/ali408mehmood-ghulam-ishaq-khan-institute-of-engineering/huggingface/runs/2xw7twqz</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2545' max='2545' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2545/2545 09:53, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.741300</td>\n      <td>0.716019</td>\n      <td>0.682399</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.533800</td>\n      <td>0.758032</td>\n      <td>0.705998</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.371200</td>\n      <td>0.843619</td>\n      <td>0.697148</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.282400</td>\n      <td>1.057486</td>\n      <td>0.694199</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.186700</td>\n      <td>1.154317</td>\n      <td>0.691249</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='128' max='128' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [128/128 00:08]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Accuracy for BERT: 0.6823992133726647\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**DistilBert**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\nfrom torch.utils.data import Dataset\nimport torch\nimport numpy as np\n\n# Load the data\nfile_path = '/kaggle/input/bbc-data-new/balanced_news_samples_max.csv'  \ndata = pd.read_csv(file_path)\n\n# Combine the 'Title' and 'Description' columns to create the 'Text' column\ndata['Text'] = data['Title'] + \" \" + data['Description']\n\n# Drop rows where 'Sentiment' or 'Text' has NaN values\ndata = data.dropna(subset=['Sentiment', 'Text'])\n\n# Encode the sentiment labels\nlabel_map = {'POSITIVE': 0, 'NEGATIVE': 1, 'NEUTRAL': 2}\ndata['Sentiment'] = data['Sentiment'].map(label_map)\n\n# After mapping, check if there are any NaN values (due to unmapped labels)\nif data['Sentiment'].isna().any():\n    print(\"There are NaN values in the 'Sentiment' column after mapping. They will be dropped.\")\n    data = data.dropna(subset=['Sentiment'])\n\n# Ensure all labels are integers\ndata['Sentiment'] = data['Sentiment'].astype(int)\n\n# Split the dataset into training and validation sets\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(data['Text'], data['Sentiment'], test_size=0.2, random_state=42)\n\n# Load the tokenizer\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n\n# Define a custom dataset class\nclass NewsDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n        item = {key: val.squeeze(0) for key, val in encoding.items()}\n        item['labels'] = torch.tensor(label, dtype=torch.long)\n        return item\n\n# Create the datasets\ntrain_dataset = NewsDataset(train_texts.tolist(), train_labels.tolist(), tokenizer)\nval_dataset = NewsDataset(val_texts.tolist(), val_labels.tolist(), tokenizer)\n\n# Load the pre-trained model\nmodel = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3)\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=5,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=10,\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    save_strategy=\"epoch\",\n)\n\n# Define evaluation metric\ndef compute_metrics(p):\n    preds = np.argmax(p.predictions, axis=1)\n    return {\"accuracy\": (preds == p.label_ids).mean()}\n\n# Initialize the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics,\n)\n\n# Fine-tune the model\ntrainer.train()\n\n# Evaluate the model\neval_result = trainer.evaluate()\n\n# Save the fine-tuned model\nmodel.save_pretrained(\"fine-tuned-distilbert\")\ntokenizer.save_pretrained(\"fine-tuned-distilbert\")\n\n# Print the evaluation results\nprint(f\"Evaluation results: {eval_result}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-13T14:31:26.418927Z","iopub.execute_input":"2024-08-13T14:31:26.419575Z","iopub.status.idle":"2024-08-13T14:37:36.186738Z","shell.execute_reply.started":"2024-08-13T14:31:26.419541Z","shell.execute_reply":"2024-08-13T14:37:36.185826Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e2a42d0de7f43d0aadfc52894e05a3b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64ea69d470c04dc5a6f4f21f4155fcac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6d2c49a2b2547e58b195905da9df5e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c3315adf09e42178eaeee1f74e2815c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd85d54e3576454f81604725b72d860c"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2545' max='2545' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2545/2545 05:58, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.559400</td>\n      <td>0.581324</td>\n      <td>0.751721</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.409000</td>\n      <td>0.621881</td>\n      <td>0.755162</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.218600</td>\n      <td>0.711199</td>\n      <td>0.757129</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.196700</td>\n      <td>0.819853</td>\n      <td>0.761554</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.165600</td>\n      <td>0.914101</td>\n      <td>0.762537</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='128' max='128' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [128/128 00:06]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation results: {'eval_loss': 0.581323504447937, 'eval_accuracy': 0.7517207472959685, 'eval_runtime': 6.7489, 'eval_samples_per_second': 301.383, 'eval_steps_per_second': 18.966, 'epoch': 5.0}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Roberta-large**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\nfrom torch.utils.data import Dataset\nimport torch\nimport numpy as np\n\n# Load the data\nfile_path = '/kaggle/input/bbc-data-new/balanced_news_samples_max.csv'  \ndata = pd.read_csv(file_path)\n\n# Combine the 'Title' and 'Description' columns to create the 'Text' column\ndata['Text'] = data['Title'] + \" \" + data['Description']\n\n# Drop rows where 'Sentiment' or 'Text' has NaN values\ndata = data.dropna(subset=['Sentiment', 'Text'])\n\n# Encode the sentiment labels\nlabel_map = {'POSITIVE': 0, 'NEGATIVE': 1, 'NEUTRAL': 2}\ndata['Sentiment'] = data['Sentiment'].map(label_map)\n\n# Ensure all labels are integers\ndata['Sentiment'] = data['Sentiment'].astype(int)\n\n# Split the dataset into training and validation sets\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(data['Text'], data['Sentiment'], test_size=0.2, random_state=42)\n\n# Load the tokenizer\ntokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n\n# Define a custom dataset class\nclass NewsDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n        item = {key: val.squeeze(0) for key, val in encoding.items()}\n        item['labels'] = torch.tensor(label, dtype=torch.long)\n        return item\n\n# Create the datasets\ntrain_dataset = NewsDataset(train_texts.tolist(), train_labels.tolist(), tokenizer)\nval_dataset = NewsDataset(val_texts.tolist(), val_labels.tolist(), tokenizer)\n\n# Load the pre-trained model\nmodel = RobertaForSequenceClassification.from_pretrained('roberta-large', num_labels=3)\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=8,  \n    per_device_eval_batch_size=8,   \n    num_train_epochs=5,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=10,\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    save_strategy=\"epoch\",\n    fp16=True,  # Use mixed precision for faster training\n)\n\n# Define evaluation metric\ndef compute_metrics(p):\n    preds = np.argmax(p.predictions, axis=1)\n    return {\"accuracy\": (preds == p.label_ids).mean()}\n\n# Initialize the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics,\n)\n\n# Fine-tune the model\ntrainer.train()\n\n# Evaluate the model\neval_result = trainer.evaluate()\n\n# Save the fine-tuned model\nmodel.save_pretrained(\"fine-tuned-roberta-large\")\ntokenizer.save_pretrained(\"fine-tuned-roberta-large\")\n\n# Print the evaluation results\nprint(f\"Evaluation results: {eval_result}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-17T21:05:52.613140Z","iopub.execute_input":"2024-08-17T21:05:52.614024Z","iopub.status.idle":"2024-08-17T21:44:57.513059Z","shell.execute_reply.started":"2024-08-17T21:05:52.613990Z","shell.execute_reply":"2024-08-17T21:44:57.512074Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5085' max='5085' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5085/5085 38:21, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.416800</td>\n      <td>0.502440</td>\n      <td>0.793019</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.426600</td>\n      <td>0.673724</td>\n      <td>0.791052</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.303500</td>\n      <td>0.887554</td>\n      <td>0.808260</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.122400</td>\n      <td>1.080547</td>\n      <td>0.807768</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.028800</td>\n      <td>1.215567</td>\n      <td>0.808260</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='255' max='255' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [255/255 00:31]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation results: {'eval_loss': 0.5024403929710388, 'eval_accuracy': 0.7930186823992134, 'eval_runtime': 31.5184, 'eval_samples_per_second': 64.534, 'eval_steps_per_second': 8.091, 'epoch': 5.0}\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification\n\n# Load the fine-tuned model and tokenizer\nmodel = RobertaForSequenceClassification.from_pretrained(\"fine-tuned-roberta-large\")\ntokenizer = RobertaTokenizer.from_pretrained(\"fine-tuned-roberta-large\")\n\n# Set the model in evaluation mode\nmodel.eval()\n\n# Define the function to predict sentiment\ndef predict_sentiment(text):\n    inputs = tokenizer(text, padding=True, truncation=True, max_length=128, return_tensors='pt')\n    with torch.no_grad():\n        outputs = model(**inputs)\n    logits = outputs.logits\n    predicted_class = torch.argmax(logits, dim=1).item()\n    label_map = {0: 'POSITIVE', 1: 'NEGATIVE', 2: 'NEUTRAL'}\n    return label_map[predicted_class]\n\n# Define a function to extract important tokens\ndef predict_important_tokens(text):\n    inputs = tokenizer(text, return_tensors='pt')\n    \n    # Get embeddings from the model's input layer\n    embeddings = model.roberta.embeddings(inputs['input_ids'])\n    embeddings.retain_grad()\n    \n    # Forward pass through the model\n    outputs = model(inputs_embeds=embeddings)\n    logits = outputs.logits\n\n    # Calculate gradients for importance scores\n    logits[:, logits.argmax(dim=-1)].backward()\n    gradients = embeddings.grad\n    token_importance = gradients.abs().sum(dim=2).squeeze()\n\n    # Get the tokens and their importance scores\n    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'].squeeze())\n    importance_scores = token_importance.detach().cpu().numpy()\n\n    # Pair tokens with their importance scores and sort by score\n    token_importance_pairs = [(token, score) for token, score in zip(tokens, importance_scores)]\n    token_importance_pairs = sorted(token_importance_pairs, key=lambda x: x[1], reverse=True)\n\n    # Clean the tokens to remove special characters and select only alphabetic tokens\n    important_tokens = [clean_token(pair[0]) for pair in token_importance_pairs[:5]]\n    return [token for token in important_tokens if token.isalpha()]\n\n# Function to clean tokens\ndef clean_token(token):\n    # Remove special characters, e.g., \"\" that indicates a space in RoBERTa tokenization\n    return token.replace('', '').replace('<s>', '').replace('</s>', '')\n\n# Test with a few news statements\nnews_statements = [\n    \"The economy is showing signs of recovery after the pandemic.\",\n    \"A massive earthquake has caused significant damage in the city.\",\n    \"The new technology product received mixed reviews from the public.\",\n    \"The government announced a new policy that is expected to benefit small businesses.\"\n]\n\nfor statement in news_statements:\n    sentiment = predict_sentiment(statement)\n    important_tokens = predict_important_tokens(statement)\n    print(f\"News: {statement}\\nPredicted Sentiment: {sentiment}\\nMost Important Tokens: {important_tokens}\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-08-17T21:52:48.859811Z","iopub.execute_input":"2024-08-17T21:52:48.860580Z","iopub.status.idle":"2024-08-17T21:52:53.928208Z","shell.execute_reply.started":"2024-08-17T21:52:48.860549Z","shell.execute_reply":"2024-08-17T21:52:53.927057Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"News: The economy is showing signs of recovery after the pandemic.\nPredicted Sentiment: POSITIVE\nMost Important Tokens: ['pand', 'emic', 'economy']\n\nNews: A massive earthquake has caused significant damage in the city.\nPredicted Sentiment: NEGATIVE\nMost Important Tokens: ['city', 'earthquake', 'significant']\n\nNews: The new technology product received mixed reviews from the public.\nPredicted Sentiment: NEUTRAL\nMost Important Tokens: ['mixed', 'reviews', 'technology', 'product', 'new']\n\nNews: The government announced a new policy that is expected to benefit small businesses.\nPredicted Sentiment: POSITIVE\nMost Important Tokens: ['government', 'businesses', 'expected', 'policy']\n\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification\n\n# Load the fine-tuned model and tokenizer\nmodel = RobertaForSequenceClassification.from_pretrained(\"fine-tuned-roberta-large\")\ntokenizer = RobertaTokenizer.from_pretrained(\"fine-tuned-roberta-large\")\n\n# Define the function to predict sentiment\ndef predict_sentiment(text):\n    inputs = tokenizer(text, padding=True, truncation=True, max_length=128, return_tensors='pt')\n    with torch.no_grad():\n        outputs = model(**inputs)\n    logits = outputs.logits\n    predicted_class = torch.argmax(logits, dim=1).item()\n    label_map = {0: 'POSITIVE', 1: 'NEGATIVE', 2: 'NEUTRAL'}\n    return label_map[predicted_class]\n\n# Test with a few news statements\nnews_statements = [\n    \"The economy is showing signs of recovery after the pandemic.\",\n    \"A massive earthquake has caused significant damage in the city.\",\n    \"The new technology product received mixed reviews from the public.\",\n    \"The government announced a new policy that is expected to benefit small businesses.\"\n]\n\nfor statement in news_statements:\n    sentiment = predict_sentiment(statement)\n    print(f\"News: {statement}\\nPredicted Sentiment: {sentiment}\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-17T21:01:55.955862Z","iopub.execute_input":"2024-08-17T21:01:55.956960Z","iopub.status.idle":"2024-08-17T21:01:58.386417Z","shell.execute_reply.started":"2024-08-17T21:01:55.956902Z","shell.execute_reply":"2024-08-17T21:01:58.385348Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"News: The economy is showing signs of recovery after the pandemic.\nPredicted Sentiment: POSITIVE\n\nNews: A massive earthquake has caused significant damage in the city.\nPredicted Sentiment: NEGATIVE\n\nNews: The new technology product received mixed reviews from the public.\nPredicted Sentiment: NEGATIVE\n\nNews: The government announced a new policy that is expected to benefit small businesses.\nPredicted Sentiment: POSITIVE\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import TFRobertaForSequenceClassification\n\n# Load the TensorFlow model (already converted from PyTorch)\ntf_model = TFRobertaForSequenceClassification.from_pretrained('fine-tuned-roberta-large', from_pt=True)\n\n# Save the model in TensorFlow's SavedModel format\ntf_model.save_pretrained(\"fine-tuned-roberta-large-tf\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-16T20:18:37.529637Z","iopub.execute_input":"2024-08-16T20:18:37.530034Z","iopub.status.idle":"2024-08-16T20:18:45.778752Z","shell.execute_reply.started":"2024-08-16T20:18:37.530007Z","shell.execute_reply":"2024-08-16T20:18:45.772915Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n\nAll the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\n\n# Function to split a large file into smaller parts\ndef split_file(file_path, chunk_size):\n    with open(file_path, 'rb') as f:\n        chunk = f.read(chunk_size)\n        i = 0\n        while chunk:\n            with open(f'{file_path}.part{i}', 'wb') as chunk_file:\n                chunk_file.write(chunk)\n            i += 1\n            chunk = f.read(chunk_size)\n\n# Compress with maximum compression and split into 500MB chunks\nshutil.make_archive('fine-tuned-roberta-large-tf', 'zip', 'fine-tuned-roberta-large-tf')\nsplit_file('fine-tuned-roberta-large-tf.zip', 500 * 1024 * 1024)  # 500MB chunks\n","metadata":{"execution":{"iopub.status.busy":"2024-08-16T21:06:53.517727Z","iopub.execute_input":"2024-08-16T21:06:53.518403Z","iopub.status.idle":"2024-08-16T21:08:30.302773Z","shell.execute_reply.started":"2024-08-16T21:06:53.518370Z","shell.execute_reply":"2024-08-16T21:08:30.301711Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"**Bert-large-uncased**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\nfrom torch.utils.data import Dataset\nimport torch\nimport numpy as np\n\n# Load the data\nfile_path = '/kaggle/input/bbc-data-new/balanced_news_samples_max.csv'  \ndata = pd.read_csv(file_path)\n\n# Combine the 'Title' and 'Description' columns to create the 'Text' column\ndata['Text'] = data['Title'] + \" \" + data['Description']\n\n# Drop rows where 'Sentiment' or 'Text' has NaN values\ndata = data.dropna(subset=['Sentiment', 'Text'])\n\n# Encode the sentiment labels\nlabel_map = {'POSITIVE': 0, 'NEGATIVE': 1, 'NEUTRAL': 2}\ndata['Sentiment'] = data['Sentiment'].map(label_map)\n\n# Ensure all labels are integers\ndata['Sentiment'] = data['Sentiment'].astype(int)\n\n# Split the dataset into training and validation sets\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(data['Text'], data['Sentiment'], test_size=0.2, random_state=42)\n\n# Load the tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n\n# Define a custom dataset class\nclass NewsDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n        item = {key: val.squeeze(0) for key, val in encoding.items()}\n        item['labels'] = torch.tensor(label, dtype=torch.long)\n        return item\n\n# Create the datasets\ntrain_dataset = NewsDataset(train_texts.tolist(), train_labels.tolist(), tokenizer)\nval_dataset = NewsDataset(val_texts.tolist(), val_labels.tolist(), tokenizer)\n\n# Load the pre-trained model\nmodel = BertForSequenceClassification.from_pretrained('bert-large-uncased', num_labels=3)\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=8,  \n    per_device_eval_batch_size=8,   \n    num_train_epochs=5,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=10,\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    save_strategy=\"epoch\",\n)\n\n# Define evaluation metric\ndef compute_metrics(p):\n    preds = np.argmax(p.predictions, axis=1)\n    return {\"accuracy\": (preds == p.label_ids).mean()}\n\n# Initialize the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics,\n)\n\n# Fine-tune the model\ntrainer.train()\n\n# Evaluate the model\neval_result = trainer.evaluate()\n\n# Save the fine-tuned model\nmodel.save_pretrained(\"fine-tuned-bert-large-uncased\")\ntokenizer.save_pretrained(\"fine-tuned-bert-large-uncased\")\n\n# Print the evaluation results\nprint(f\"Evaluation results: {eval_result}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-13T17:13:35.649032Z","iopub.execute_input":"2024-08-13T17:13:35.649399Z","iopub.status.idle":"2024-08-13T17:52:25.863660Z","shell.execute_reply.started":"2024-08-13T17:13:35.649364Z","shell.execute_reply":"2024-08-13T17:52:25.862621Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-08-13 17:13:45.720791: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-13 17:13:45.720905: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-13 17:13:45.863291: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa69fc702bc1459296574a26eb1e9f27"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1132a45e11c48dbbd9f687765198624"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bb4514d7def4beea67bb0b31e61beaf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"960b9b8d786045eda9032cd9aa2ce836"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57553106d0694d5ca642ce26bf02f182"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240813_171429-5z8fbfc8</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ali408mehmood-ghulam-ishaq-khan-institute-of-engineering/huggingface/runs/5z8fbfc8' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/ali408mehmood-ghulam-ishaq-khan-institute-of-engineering/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ali408mehmood-ghulam-ishaq-khan-institute-of-engineering/huggingface' target=\"_blank\">https://wandb.ai/ali408mehmood-ghulam-ishaq-khan-institute-of-engineering/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ali408mehmood-ghulam-ishaq-khan-institute-of-engineering/huggingface/runs/5z8fbfc8' target=\"_blank\">https://wandb.ai/ali408mehmood-ghulam-ishaq-khan-institute-of-engineering/huggingface/runs/5z8fbfc8</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5085' max='5085' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5085/5085 37:05, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.476600</td>\n      <td>0.538716</td>\n      <td>0.764503</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.248200</td>\n      <td>0.741872</td>\n      <td>0.776794</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.138000</td>\n      <td>0.990956</td>\n      <td>0.786136</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.108100</td>\n      <td>1.189167</td>\n      <td>0.786627</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.000800</td>\n      <td>1.303304</td>\n      <td>0.793019</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='255' max='255' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [255/255 00:29]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation results: {'eval_loss': 0.5387162566184998, 'eval_accuracy': 0.7645034414945919, 'eval_runtime': 30.1041, 'eval_samples_per_second': 67.566, 'eval_steps_per_second': 8.471, 'epoch': 5.0}\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\nfrom torch.utils.data import Dataset\nimport torch\nimport numpy as np\n\n# Load the data\nfile_path = '/kaggle/input/bbc-data-new/balanced_news_samples_max.csv'\ndata = pd.read_csv(file_path)\n\n# Combine the 'Title' and 'Description' columns to create the 'Text' column\ndata['Text'] = data['Title'] + \" \" + data['Description']\n\n# Drop rows where 'Sentiment' or 'Text' has NaN values\ndata = data.dropna(subset=['Sentiment', 'Text'])\n\n# Encode the sentiment labels\nlabel_map = {'POSITIVE': 0, 'NEGATIVE': 1, 'NEUTRAL': 2}\ndata['Sentiment'] = data['Sentiment'].map(label_map)\n\n# Ensure all labels are integers\ndata['Sentiment'] = data['Sentiment'].astype(int)\n\n# Split the dataset into training and validation sets\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(data['Text'], data['Sentiment'], test_size=0.2, random_state=42)\n\n# Load the tokenizer\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n\n# Define a custom dataset class\nclass NewsDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n        item = {key: val.squeeze(0) for key, val in encoding.items()}\n        item['labels'] = torch.tensor(label, dtype=torch.long)\n        return item\n\n# Create the datasets\ntrain_dataset = NewsDataset(train_texts.tolist(), train_labels.tolist(), tokenizer)\nval_dataset = NewsDataset(val_texts.tolist(), val_labels.tolist(), tokenizer)\n\n# Load the pre-trained model\nmodel = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=3)\n\n# Define training arguments with weight decay and other regularization techniques\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=8,  \n    per_device_eval_batch_size=8,   \n    num_train_epochs=5,\n    weight_decay=0.01,  # Regularization term\n    logging_dir='./logs',\n    logging_steps=10,\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    save_strategy=\"epoch\",\n    fp16=True,  # Use mixed precision for faster training\n)\n\n# Define evaluation metric\ndef compute_metrics(p):\n    preds = np.argmax(p.predictions, axis=1)\n    return {\"accuracy\": (preds == p.label_ids).mean()}\n\n# Initialize the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics,\n)\n\n# Fine-tune the model\ntrainer.train()\n\n# Evaluate the model\neval_result = trainer.evaluate()\n\n# Save the fine-tuned model\nmodel.save_pretrained(\"fine-tuned-roberta-base\")\ntokenizer.save_pretrained(\"fine-tuned-roberta-base\")\n\n# Print the evaluation results\nprint(f\"Evaluation results: {eval_result}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-13T18:49:26.684510Z","iopub.execute_input":"2024-08-13T18:49:26.685439Z","iopub.status.idle":"2024-08-13T19:02:32.374947Z","shell.execute_reply.started":"2024-08-13T18:49:26.685398Z","shell.execute_reply":"2024-08-13T19:02:32.373918Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5085' max='5085' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5085/5085 12:52, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.417500</td>\n      <td>0.581895</td>\n      <td>0.778761</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.364700</td>\n      <td>0.734121</td>\n      <td>0.777778</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.092900</td>\n      <td>0.918367</td>\n      <td>0.798427</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.183900</td>\n      <td>1.156674</td>\n      <td>0.796460</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.010300</td>\n      <td>1.252180</td>\n      <td>0.797443</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='255' max='255' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [255/255 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation results: {'eval_loss': 0.5818952322006226, 'eval_accuracy': 0.7787610619469026, 'eval_runtime': 10.908, 'eval_samples_per_second': 186.468, 'eval_steps_per_second': 23.377, 'epoch': 5.0}\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}